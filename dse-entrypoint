#!/bin/sh
# Bind the various services
# These should be updated on every container start
if [ -z ${IP} ]; then
 IP=`hostname --ip-address`
fi
echo $IP > /data/ip.address
printf "using IP address: "$IP
# create directories for holding the node's data, logs, etc.
create_dirs() {
 local base_dir=$1;
 printf "Going to make directories....\n"
 mkdir -p $base_dir/data/commitlog
 mkdir -p $base_dir/data/saved_caches
 mkdir -p $base_dir/logs
 printf "Made directories.... \n"
}
# copy the relevant sections of the config for a service (includes bin for
# selected services like cassandra or hadoop)
copy_config() {
 base_dst_dir=$1
 base_dir=$2
 service=$3
 src="$base_dir/resources/$service/conf"
 dst="$base_dst_dir/$service"
 cp -r $src $dst
}
# tweak the cassandra config
tweak_cassandra_config() {
 env="$1/cassandra-env.sh"
 conf="$1/cassandra.yaml"
 base_data_dir="/data"
 # Set the cluster name
 if [ -z "${CLUSTER_NAME}" ]; then
 printf " - No cluster name provided; skipping.\n"
 else

 printf " - Setting up the cluster name: ${CLUSTER_NAME}\n"
 regexp="s/Test Cluster/${CLUSTER_NAME}/g"
 sed -i -- "$regexp" $conf
 fi
 # Set the commitlog directory, and various other directories
 # These are done only once since the regexep matches will fail on subsequent
 # runs.
 printf " - Setting up directories\n"
 regexp="s|/var/lib/cassandra/|$base_data_dir/|g"
 sed -i -- "$regexp" $conf
 regexp="s/^listen_address:.*/listen_address: ${IP}/g"
 sed -i -- "$regexp" $conf
 regexp="s/rpc_address:.*/rpc_address: ${IP}/g"
 sed -i -- "$regexp" $conf
 # seeds
 if [ -z "${SEEDS}" ]; then
 printf " - Using own IP address ${IP} as seed.\n";
 regexp="s/seeds:.*/seeds: \"${IP}\"/g";
 else
 printf " - Using seeds: $SEEDS\n";
 regexp="s/seeds:.*/seeds: \"${IP},${SEEDS}\"/g"
 fi
 sed -i -- "$regexp" $conf
 # JMX
 echo "JVM_OPTS=\"\$JVM_OPTS -Djava.rmi.server.hostname=127.0.0.1\"" >> $env
}
tweak_spark_config() {
 sed -i -- "s|/var/lib/spark/|/data/spark/|g" $1/spark-env.sh
 sed -i -- "s|/var/log/spark/|/logs/spark/|g" $1/spark-env.sh
 mkdir -p /data/spark/worker
 mkdir -p /data/spark/rdd
 mkdir -p /logs/spark/worker
}
tweak_agent_config() {
[ -d "$OPSC_ADDR_DIR" ] && cat > $OPSC_ADDR_DIR/address.yaml <<EOF
cassandra_conf: /conf/cassandra/cassandra.yaml
local_interface: ${IP}
hosts: ["${IP}"]
cassandra_install_location: /opt/dse
cassandra_log_location: /logs
EOF
}
setup_resources() {
 printf " - Copying configs\n"
 copy_config $1 $2 "cassandra"
 copy_config $1 $2 "dse"
 copy_config $1 $2 "hadoop"
 copy_config $1 $2 "pig"
 copy_config $1 $2 "hive"
 copy_config $1 $2 "sqoop"
 copy_config $1 $2 "mahout"
 copy_config $1 $2 "spark"
 copy_config $1 $2 "shark"
 copy_config $1 $2 "tomcat"
 printf "finished copying configs\n"
}
setup_node() {
 printf "* Setting up node...\n"
 printf " + Setting up node...\n"
 create_dirs
 printf "created directories...\n"
 setup_resources "/conf" ${DSE_HOME}
 printf "setup resources...\n"
 tweak_cassandra_config "/conf/cassandra"
 printf "tweaked cassandra config...\n"
 tweak_spark_config "/conf/spark"
 printf "tweaked spark config...\n"
 tweak_agent_config
 printf "tweaked agent config...\n"
 chown -R dse:dse /data /logs /conf
 printf "Done.\n"
}
# if conf dir is empty, generate config
[ -z "$(ls -A $CONF_DIR)" ] && setup_node
exec gosu dse $DSE_HOME/bin/dse cassandra -f "$@"


